---
title: 'Blog Post number 1'
date: 2020-02-15
permalink: /posts/2020/02/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

# Application and Algorithm of Inverse Reinforcement Learning(IRL) in self-driving car


## Overview

I did some projects about self-driving car in the [MSC](https://yangcyself.github.io/) lab UC Berkeley. Here by I summaries what I learned from this field, and briefly foreshadow what we did in the projects.

## Background in self-driving car

A good self-driving car has to ensure two aspects: comfort to passenger, and etiquette in Interaction, and most importantly safety. The first two espects requires a similar driving style to a human driver, and the last two need the car to predict the human drivers' behaviors. Thus, the essence in these problems is learning the behaviors from human, which is the topic of IRL. In the scenerio of self-driving car, some people post the problems as optimial control instead of reinforcement learning by ignoring the probabilistics in the transition of the system. So, some related works are named as Inverse Optimal Control(IOC).

To future abstract the problem of self-driving, we only study the trajectories of the car and have the following notations.

- Trajectory: $\zeta \in S$
- Feature: $f \in S \rightarrow R$
- Feature Count: $\boldsymbol{f} \in \mathrm{S} \rightarrow R^{k}$
- Weights: $\theta \in R^{k}$
- Cost(reward): $\theta^{T} \boldsymbol{f}(\zeta)$

Features are human defined characteristics of the trajectory. For example the `a_lon`: longitunal accleration of the whole trajectory(The L2 norm of the accleration at each time point). For convinence, we let the features to represent an aspect of "cost" of the trajectory, so that the weights can have equal signs.

In the projects I participated in, we assume the Cost function is some linear combination of the features. Thus, each feature has a weight corresponding to it. Note that this assumption also made it important for feature engineering(`L2_a_lon` is quite different from `L1_a_lon`).

A tricky problem of the IRL is how to judge the algorithm actually learned something. The learning goal is to **match the feature counts** of the human trajectories and predicted trajectories.
$$
E_{P(\zeta | \theta)}[\boldsymbol{f}]=\tilde{\boldsymbol{f}}
$$


To compute the $E$ in the above equation, the max entropy principle is often used:
$$
P(\zeta | \theta)=\frac{1}{z} e^{-\theta^{T}} f(\zeta)
$$

> If you are curious about the assumptions behind the learning gaol and the max entropy principle, please check [background understanding](#background-understanding) section

## Learning algorithms



## background understanding

# References
